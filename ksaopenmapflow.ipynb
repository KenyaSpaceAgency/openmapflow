{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5133d90-4eb9-4e0c-a986-b353e7b0d32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1929613691554158b5d44ea44171fca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='10em', width='50%'), placeholder='openmapflow.yaml')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "config_yml_input = widgets.Textarea(placeholder=\"openmapflow.yaml\", layout=widgets.Layout(height=\"10em\", width=\"50%\"))\n",
    "config_yml_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c361ecb5-fba6-4f9a-a080-1f8879c27663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('openmapflow.yaml', 'w') as f:\n",
    "#   f.write(config_yml_input.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532bf461-a884-48f9-b13d-14542a9c9b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openmapflow_local\n",
      "Azure Storage client initialized with managed identity.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cmocean\n",
    "import rasterio as rio\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import pickle  # For loading/saving models\n",
    "import numpy as np\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "PROJECT = \"openmapflow_local\"\n",
    "print(PROJECT)\n",
    "\n",
    "# Azure Storage Configuration\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    blob_service_client = BlobServiceClient(\n",
    "        account_url=\"https://openmapflow.blob.core.windows.net/\", credential=credential\n",
    "    )\n",
    "    container_name = \"openmap\"\n",
    "    print(\"Azure Storage client initialized with managed identity.\")\n",
    "except Exception as e:\n",
    "    try:\n",
    "        AZURE_STORAGE_CONNECTION_STRING = os.environ.get(\n",
    "            \"AZURE_STORAGE_CONNECTION_STRING\"\n",
    "        )\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            AZURE_STORAGE_CONNECTION_STRING\n",
    "        )\n",
    "        container_name = \"openmap\"\n",
    "        print(\"Azure Storage client initialized with connection string.\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Azure Storage connection failed: {e}, {e2}\")\n",
    "        blob_service_client = None\n",
    "        container_name = None\n",
    "\n",
    "# Azure Blob Download\n",
    "def download_from_azure(blob_name, local_file_path):\n",
    "    \"\"\"Downloads a blob from Azure Blob Storage.\"\"\"\n",
    "    if blob_service_client is None:\n",
    "        print(\"Azure Storage not configured.\")\n",
    "        return False\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, blob=blob_name\n",
    "        )\n",
    "        with open(local_file_path, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "        print(f\"Downloaded {blob_name} to {local_file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {blob_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Azure Blob Upload\n",
    "def upload_to_azure(local_file_path, blob_name):\n",
    "    \"\"\"Uploads a local file to Azure Blob Storage.\"\"\"\n",
    "    if blob_service_client is None:\n",
    "        print(\"Azure Storage not configured.\")\n",
    "        return False\n",
    "    try:\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, blob=blob_name\n",
    "        )\n",
    "        with open(local_file_path, \"rb\") as data:\n",
    "            blob_client.upload_blob(data, overwrite=True)\n",
    "        print(f\"Uploaded {local_file_path} to {blob_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading {local_file_path} to {blob_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Prediction Logic (Example)\n",
    "def make_new_predictions(input_blob_name, model_blob_name, output_blob_name):\n",
    "    \"\"\"\n",
    "    Makes predictions using a model and input data from Azure Blob Storage.\n",
    "\n",
    "    Args:\n",
    "        input_blob_name (str): Name of the input data blob.\n",
    "        model_blob_name (str): Name of the model blob.\n",
    "        output_blob_name (str): Name of the output prediction blob.\n",
    "    \"\"\"\n",
    "    local_input_path = \"input.tif\"\n",
    "    local_model_path = \"data/models/ksaopenmap.pt\"\n",
    "    local_output_path = \"output.tif\"\n",
    "\n",
    "    if not download_from_azure(input_blob_name, local_input_path):\n",
    "        return False\n",
    "    if not download_from_azure(model_blob_name, local_model_path):\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        with open(local_model_path, \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "\n",
    "        with rio.open(local_input_path) as src:\n",
    "            data = src.read()\n",
    "            profile = src.profile\n",
    "            # Example prediction: replace with your model's prediction logic\n",
    "            predictions = model.predict(data.reshape(data.shape[0], -1).T).reshape(data.shape[1],data.shape[2])\n",
    "            predictions = np.expand_dims(predictions, axis=0) #add a channel dimension.\n",
    "\n",
    "            profile.update(\n",
    "                dtype=rio.float32,\n",
    "                count=1,\n",
    "                compress='lzw'\n",
    "            )\n",
    "\n",
    "        with rio.open(local_output_path, 'w', **profile) as dst:\n",
    "            dst.write(predictions.astype(rio.float32))\n",
    "\n",
    "        if upload_to_azure(local_output_path, output_blob_name):\n",
    "            print(f\"Predictions saved to {output_blob_name}\")\n",
    "            os.remove(local_input_path)\n",
    "            os.remove(local_model_path)\n",
    "            os.remove(local_output_path)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        return False\n",
    "\n",
    "# Example Usage:\n",
    "# make_new_predictions(\"input_data.tif\", \"my_model.pkl\", \"predictions.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d604ce4-27c1-4fb9-a6c4-64f458e7f434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshua/openmapflow/.venv/lib/python3.12/site-packages/torch/serialization.py:1328: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ksaopenmapflow.pt']\n",
      "Model ksaopenmapflow.pt loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch  # Import PyTorch\n",
    "\n",
    "def get_available_models(model_directory):\n",
    "    \"\"\"Retrieves available models from a local directory.\"\"\"\n",
    "    try:\n",
    "        models = []\n",
    "        for filename in os.listdir(model_directory):\n",
    "            if filename.endswith(\".pt\"):  # Check for .pt files\n",
    "                model_path = os.path.join(model_directory, filename)\n",
    "                try:\n",
    "                    torch.load(model_path, map_location=torch.device('cpu')) #attempt to load the model.\n",
    "                    models.append(filename) #if successful, add it to the list.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading model {filename}: {e}\")\n",
    "        return models\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory not found: {model_directory}\")\n",
    "        return []\n",
    "\n",
    "def load_model(model_filename, model_directory):\n",
    "    \"\"\"Loads a PyTorch model from a local file.\"\"\"\n",
    "    model_path = os.path.join(model_directory, model_filename)\n",
    "    try:\n",
    "        model = torch.load(model_path, map_location=torch.device('cpu')) #map to cpu for cross platform loading.\n",
    "        return model\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        return None\n",
    "\n",
    "# Set the directory where your models are stored\n",
    "model_directory = \"data/models/\"  # Replace with your actual directory\n",
    "\n",
    "available_models = get_available_models(model_directory)\n",
    "\n",
    "if available_models:\n",
    "    print(available_models)\n",
    "    model_name = available_models[0] #load first model.\n",
    "    loaded_model = load_model(model_name, model_directory)\n",
    "\n",
    "    if loaded_model:\n",
    "        print(f\"Model {model_name} loaded successfully.\")\n",
    "        # Use the loaded PyTorch model for predictions\n",
    "        # For example, if you are doing inference.\n",
    "        # loaded_model.eval() #set the model to evaluation mode.\n",
    "        # with torch.no_grad(): #disable gradient calculations.\n",
    "        #    predictions = loaded_model(input_tensor) #input_tensor would be your input data.\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to load model {model_name}.\")\n",
    "\n",
    "else:\n",
    "    print(\"No models found in the specified directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0341f92b-2ac3-4f12-9145-7c61e901b9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Bounding Boxes:\n",
      "CustomBBox(min_lat=-0.95, max_lat=-0.55, min_lon=36.75, max_lon=37.35, name='from_file')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from openmapflow.bbox import BBox  # Assuming you have a BBox class\n",
    "\n",
    "@dataclass\n",
    "class CustomBBox(BBox):\n",
    "    name: str = \"from_file\" #add a name, as it is required.\n",
    "\n",
    "def extract_bbox_from_filename(filename):\n",
    "    \"\"\"Extracts bounding box coordinates from a filename.\"\"\"\n",
    "    pattern = r\"min_lat=(-?\\d+\\.?\\d*)_min_lon=(-?\\d+\\.?\\d*)_max_lat=(-?\\d+\\.?\\d*)_max_lon=(-?\\d+\\.?\\d*)\"\n",
    "    match = re.search(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        min_lat = float(match.group(1))\n",
    "        min_lon = float(match.group(2))\n",
    "        max_lat = float(match.group(3))\n",
    "        max_lon = float(match.group(4))\n",
    "        return CustomBBox(\n",
    "            min_lat=min_lat,\n",
    "            min_lon=min_lon,\n",
    "            max_lat=max_lat,\n",
    "            max_lon=max_lon,\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def generate_bboxes_from_directory(directory):\n",
    "    bboxes = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".tif\") and filename.startswith(\"min_lat=\"):\n",
    "            bbox = extract_bbox_from_filename(filename)\n",
    "            if bbox:\n",
    "                bboxes.append(bbox)\n",
    "    return bboxes\n",
    "\n",
    "# Replace \"./your_directory\" with the actual path to your directory\n",
    "directory = \"./openmapflow/satdata/\"  # or \"data/training_data\" or whatever your directory is.\n",
    "available_bboxes = generate_bboxes_from_directory(directory)\n",
    "\n",
    "if available_bboxes:\n",
    "    print(\"Available Bounding Boxes:\")\n",
    "    for bbox in available_bboxes:\n",
    "        print(bbox)\n",
    "else:\n",
    "    print(\"No bounding boxes found in the directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84b00c7-a5b6-4484-8af7-004ca78a5dd0",
   "metadata": {},
   "source": [
    "##inference configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a55ed49-5670-4a8d-9c50-354e99caec37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4a8855855c44b4bf3f689b75869f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Box(children=(VBox(children=(HTML(value='<h3>Select model and specify region of interest</h3>')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from openmapflow.inference_widgets import InferenceWidget\n",
    "inference_widget = InferenceWidget(available_models=available_models, available_bboxes=available_bboxes)\n",
    "inference_widget.ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a59337-f3c4-4088-ad86-99dc73280b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using map key: ksaopenmapflow.pt/min_lat=-0.95_min_lon=36.75_max_lat=-0.55_max_lon=37.35_dates=2020-02-01_2021-02-01_all\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient  # For Azure Blob Storage\n",
    "import os\n",
    "# Assuming you have inference_widget and available_models defined elsewhere\n",
    "\n",
    "# Azure Configuration\n",
    "AZURE_STORAGE_CONNECTION_STRING = os.environ.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "AZURE_CONTAINER_PREDS_MERGED = os.environ.get(\"AZURE_CONTAINER_PREDS_MERGED\")\n",
    "# Local Directory Configuration\n",
    "LOCAL_PREDS_MERGED_PATH = \"./local_preds_merged\" # Replace with your local path.\n",
    "AZURE_STORAGE_CONNECTION_STRING\n",
    "\n",
    "config = inference_widget.get_config_as_dict()\n",
    "map_key = config[\"map_key\"]\n",
    "bbox = config[\"bbox\"]\n",
    "start_date = config[\"start_date\"]\n",
    "end_date = config[\"end_date\"]\n",
    "tifs_in_gcloud = config[\"tifs_in_gcloud\"]\n",
    "\n",
    "def get_map_files(map_key):\n",
    "    map_files = []\n",
    "    if AZURE_STORAGE_CONNECTION_STRING and AZURE_CONTAINER_PREDS_MERGED:\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "        container_client = blob_service_client.get_container_client(AZURE_CONTAINER_PREDS_MERGED)\n",
    "        blobs = container_client.list_blobs(name_starts_with=f\"{map_key}.tif\")\n",
    "        map_files.extend([f\"az://{AZURE_CONTAINER_PREDS_MERGED}/{blob.name}\" for blob in blobs])\n",
    "    return map_files\n",
    "\n",
    "existing_map_files = get_map_files(map_key)\n",
    "while len(existing_map_files) > 0:\n",
    "    print(f\"Map for {map_key} already exists: \\n{existing_map_files}\")\n",
    "    map_key += \"_\" + input(f\"Append to map key: {map_key}_\")\n",
    "    existing_map_files = get_map_files(map_key)\n",
    "\n",
    "print(f\"Using map key: {map_key}\")\n",
    "# Now you can use the unique map_key for your further processing.\n",
    "\n",
    "# export AZURE_STORAGE_ACCOUNT_NAME=openmapflow\n",
    "# export AZURE_STORAGE_ACCOUNT_KEY=gBh30r5wqeU2HMhfG5jTmG0Ags++3rsYe1wTotQoxNK/EVnCnBCOt7ytHQrJuBya9/qMT/63xE3k+ASth7eOBQ==                                                               export AZURE_CONTAINER_NAME=openmap\n",
    "# export MODELS_API_URL=\"./data/models\"\n",
    "# export AZURE_INFERCONTAINER_NAME=inference-eo-container\n",
    "# export AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;EndpointSuffix=core.windows.net;AccountName=openmapflow;AccountKey=gBh30r5wqeU2HMhfG5jTmG0Ags++3rsYe1wTotQoxNK/EVnCnBCOt7ytHQrJuBya9/qMT/63xE3k+ASth7eOBQ==;BlobEndpoint=https://openmapflow.blob.core.windows.net/;FileEndpoint=https://openmapflow.file.core.windows.net/;QueueEndpoint=https://openmapflow.queue.core.windows.net/;TableEndpoint=https://openmapflow.table.core.windows.net/\n",
    "\n",
    "# export AZURE_CONTAINER_PREDS_MERGED=preds-merged-container\n",
    "                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8993989-25ec-4c5e-ae62-72fcd97df52f",
   "metadata": {},
   "source": [
    "#run first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80a7afa-6e9b-463c-9326-688790030172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "\n",
    "def inference_status(map_key, existing_map_files, local_tifs, preds_in_azure):\n",
    "    \"\"\"\n",
    "    Checks inference status based on TIFF files stored locally and in Azure Blob Storage.\n",
    "    Handles file uploads, movement, and inference status checks.\n",
    "    Provides detailed logging of map_key, file locations, and transformations.\n",
    "\n",
    "    :param map_key: Unique identifier for the map\n",
    "    :param existing_map_files: List of merged map files in `preds-merged-container`\n",
    "    :param local_tifs: List of local TIFF files available for processing\n",
    "    :param preds_in_azure: List of TIFF files in `preds-container` on Azure\n",
    "    :return: Status message indicating the next step\n",
    "    \"\"\"\n",
    "    print(f\"Inference Status Check for map_key: {map_key}\") # log the map_key\n",
    "\n",
    "    tifs_amount, predictions_amount = get_status(map_key)\n",
    "\n",
    "    # Check if merged map already exists\n",
    "    if existing_map_files:\n",
    "        print(f\"Merged map found in Azure: {existing_map_files}\")\n",
    "        return f\"Merged map available in Azure: {existing_map_files}\"\n",
    "\n",
    "    # Check if inference is complete\n",
    "    if tifs_amount > 0 and tifs_amount == predictions_amount:\n",
    "        print(f\"Inference complete. Total TIFs: {tifs_amount}, Predictions: {predictions_amount}\")\n",
    "        return \"Inference complete! Time to merge predictions into a map.\"\n",
    "\n",
    "    # Retry missing predictions\n",
    "    if tifs_amount > predictions_amount:\n",
    "        print(f\"Inference incomplete. Total TIFs: {tifs_amount}, Predictions: {predictions_amount}\")\n",
    "        if confirmation(\"Predictions in progress but incomplete. Retry missing predictions? (y/n)\"):\n",
    "            missing = find_missing_predictions(map_key)\n",
    "            print(f\"Missing predictions found: {missing}\") # log missing files\n",
    "            make_new_predictions(missing)\n",
    "            print(\"Retrying model on missing predictions...\")\n",
    "            return \"Retrying model on missing predictions...\"\n",
    "        else:\n",
    "            print(\"Waiting for predictions to complete.\")\n",
    "            return \"Waiting for predictions to complete.\"\n",
    "\n",
    "    # Move TIFFs within Azure if misplaced\n",
    "    if preds_in_azure:\n",
    "        dest_container = AZURE_CONTAINER_PREDS_MERGED\n",
    "        print(f\"TIFFs found in incorrect Azure container: {preds_in_azure}\") # log found files\n",
    "        if confirmation(f\"Move TIFFs to {dest_container}?\"):\n",
    "            move_tifs_in_azure(preds_in_azure, dest_container, map_key)\n",
    "            print(f\"Moved TIFFs to {dest_container}\")\n",
    "            return get_status(map_key)\n",
    "\n",
    "    # Upload local TIFFs to Azure if no existing data is found\n",
    "    if not preds_in_azure and local_tifs:\n",
    "        print(f\"Local TIFFs found: {local_tifs}\") # log local files\n",
    "        if confirmation(\"No existing predictions found in Azure. Upload local TIFFs?\"):\n",
    "            upload_tifs_to_azure(local_tifs, AZURE_CONTAINER_PREDS_MERGED, map_key)\n",
    "            print(f\"Uploaded {len(local_tifs)} TIFFs to Azure container {AZURE_CONTAINER_PREDS_MERGED}\")\n",
    "            return f\"Uploading {len(local_tifs)} TIFFs to Azure container {AZURE_CONTAINER_PREDS_MERGED}...\"\n",
    "\n",
    "    print(\"No data available for inference.\")\n",
    "    return \"No data available for inference. Provide TIFF files locally or in Azure.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf652258-3ad1-4805-b7a3-098b5e3f19e9",
   "metadata": {},
   "source": [
    "Merge predictions into a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "214362b3-0e9b-4bab-b70d-27e5a6163794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: /home/joshua/openmapflow/openmapflow.yaml\n",
      "Config loaded: {'project': 'openmapflow_project', 'data_paths': {'raw_labels': '/data/raw_labels', 'datasets': '/data/datasets', 'models': '/data/models', 'metrics': '/data/metrics', 'report': '/data/report'}, 'azure': {'labeled_eo_container': 'openmap', 'inference_eo_container': 'inference-eo-container', 'preds_container': 'preds-container', 'preds_merged_container': 'preds-merged-container'}}\n",
      "Download predictions as nc files (may take several minutes)\n",
      "Building vrt for each batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix for final VRT: ksaopenmapflow.pt_min_lat=-0.95_min_lon=36.75_max_lat=-0.55_max_lon=37.35_dates=2020-02-01_2021-02-01_all\n",
      "Building full vrt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...10...20...30...40...50...60...70...80...90...100 - done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: Can't open ksaopenmapflow.pt_min_lat=-0.95_min_lon=36.75_max_lat=-0.55_max_lon=37.35_dates=2020-02-01_2021-02-01_all_vrts/*.vrt. Skipping it\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.core.exceptions import AzureError\n",
    "from openmapflow.inference_utils import build_vrt\n",
    "\n",
    "# Get Azure Storage account details from environment variables\n",
    "storage_account_name = os.environ.get(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "storage_account_key = os.environ.get(\"AZURE_STORAGE_ACCOUNT_KEY\")\n",
    "container_name = os.environ.get(\"AZURE_CONTAINER_PREDS\")\n",
    "\n",
    "#error checking for environment variables\n",
    "if not storage_account_name or not storage_account_key or not container_name:\n",
    "    print(\"Error: Missing Azure Storage environment variables.\")\n",
    "    exit(1)\n",
    "\n",
    "# Check if map_key is defined\n",
    "try:\n",
    "    map_key\n",
    "except NameError:\n",
    "    print(\"Error: map_key is not defined. Please ensure it is defined in a previous cell.\")\n",
    "    exit(1)\n",
    "\n",
    "# Create Azure Blob Service client\n",
    "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "try:\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "except ValueError as e:\n",
    "    print(f\"Error creating BlobServiceClient: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create local directories\n",
    "prefix = map_key.replace(\"/\", \"_\")\n",
    "Path(f\"{prefix}_preds\").mkdir(exist_ok=True)\n",
    "Path(f\"{prefix}_vrts\").mkdir(exist_ok=True)\n",
    "Path(f\"{prefix}_tifs\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Download predictions as nc files (may take several minutes)\")\n",
    "source_prefix = f\"{map_key}\"\n",
    "destination_folder = f\"{prefix}_preds\"\n",
    "\n",
    "# List and download blobs with the given prefix\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "try:\n",
    "    blobs = container_client.list_blobs(name_starts_with=source_prefix)\n",
    "except AzureError as e:\n",
    "    print(f\"Error listing blobs: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Download each blob\n",
    "for blob in blobs:\n",
    "    relative_path = blob.name[len(source_prefix):].lstrip('/')\n",
    "    destination_path = os.path.join(destination_folder, relative_path)\n",
    "    os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(destination_path):\n",
    "        print(f\"Skipping existing file: {destination_path}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Downloading {blob.name}...\")\n",
    "    blob_client = container_client.get_blob_client(blob.name)\n",
    "    try:\n",
    "        with open(destination_path, \"wb\") as download_file:\n",
    "            download_file.write(blob_client.download_blob().readall())\n",
    "    except (AzureError, OSError) as e:\n",
    "        print(f\"Error downloading {blob.name}: {e}\")\n",
    "\n",
    "# Call the build_vrt function after downloads complete\n",
    "try:\n",
    "    build_vrt(prefix)\n",
    "except Exception as e:\n",
    "    print(f\"Error building VRT: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85d0ee7-9a71-4f1e-acff-101e9eeebac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
